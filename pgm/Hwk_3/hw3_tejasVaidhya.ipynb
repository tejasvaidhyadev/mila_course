{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKIz_ttKiG_1"
      },
      "source": [
        "# IFT6269 - Homework 3 - EM and Gaussian mixtures\n",
        "**Due:**  Tuesday, November 8, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whO8SD53Y9Vl"
      },
      "source": [
        "#### Name: Tejas Vaidhya\n",
        "#### Student ID: uv56320\n",
        "#### Collaborators: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02fg3bxiZOMv"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The file `EMGaussian.train` contains samples of data $\\{x_n\\}_{n=1}^N$ where $x_n \\in \\mathbb{R}^2$, with one datapoint per row. The goal of this exercise is to implement the K-Means and EM algorithms using $K=4$ components/clusters. \n",
        "\n",
        "### Tasks\n",
        "0.   Get your own copy of this file via \"File > Save a copy in Drive...\",\n",
        "1.   Fill your personal information and collaborators at the top of this assignment, and rename the notebook accordingly, e.g., `hw3_thomasBayes.ipynb`\n",
        "2.   Read the instructions provided on each section and cell carefully,\n",
        "4.   Implement the requested algorithms in section **Playground**\n",
        "5.   In section **Model Comparison**, simply execute the cells (without changing the code) and type your answers to the questions in the provided text cells.\n",
        "    \n",
        "**Important**: You are allowed to collaborate with other students in both the math and coding parts of this assignment. However, the answers provided here must reflect your individual work. For that reason, you are not allowed to share this notebook, except for your submission to the TA for grading. **Don't forget to pin and save the version of the notebook you want to be graded on!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3gk4JHNY9yW",
        "outputId": "2f66fbfa-3a7f-4cca-c0f3-08ee3585b027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-24 02:12:40--  http://www.iro.umontreal.ca/~slacoste/teaching/ift6269/A21/notes/hwk3data.zip\n",
            "Resolving www.iro.umontreal.ca (www.iro.umontreal.ca)... 132.204.26.36\n",
            "Connecting to www.iro.umontreal.ca (www.iro.umontreal.ca)|132.204.26.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7269 (7.1K) [application/zip]\n",
            "Saving to: ‘hwk3data.zip’\n",
            "\n",
            "\rhwk3data.zip          0%[                    ]       0  --.-KB/s               \rhwk3data.zip        100%[===================>]   7.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-24 02:12:41 (647 MB/s) - ‘hwk3data.zip’ saved [7269/7269]\n",
            "\n",
            "Archive:  hwk3data.zip\n",
            "  inflating: hwk3data/EMGaussian.test  \n",
            "  inflating: hwk3data/EMGaussian.train  \n"
          ]
        }
      ],
      "source": [
        "!wget http://www.iro.umontreal.ca/~slacoste/teaching/ift6269/A21/notes/hwk3data.zip\n",
        "!unzip -o hwk3data.zip\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "X_train = np.loadtxt(\"/content/hwk3data/EMGaussian.train\")\n",
        "X_test = np.loadtxt(\"/content/hwk3data/EMGaussian.test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxkIdgdXjIdr"
      },
      "source": [
        "## Playground\n",
        "\n",
        "You are allowed to add as many cells and functions as you wish in this section, but not allowed to change the signature (name and inputs) of the functions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruT7pHDWu0xW"
      },
      "outputs": [],
      "source": [
        "#@title Plotting code _(execute ▸ , but do not modify)_\n",
        "# ---------------------------------------------------------------------------\n",
        "#                       Code for plotting the results \n",
        "#                      ! DO NOT MODIFY THESE FUNCTIONS !\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "def plot_ellipse(ax, mu, sigma, alpha=1, color=\"k\"):\n",
        "    evals, evecs = np.linalg.eigh(sigma)\n",
        "    x, y = evecs[:, 0]\n",
        "    theta = np.degrees(np.arctan2(y, x))\n",
        "    for factor in [1.5, 3]:\n",
        "        w, h = factor * np.sqrt(evals)\n",
        "        ellipse = Ellipse(mu, w, h, theta, facecolor='none', edgecolor=color) \n",
        "        ellipse.set_alpha(alpha)\n",
        "        ax.add_artist(ellipse) \n",
        "        \n",
        "def show_kmeans(X_train, X_test, KM_centroids, KM_predictor):\n",
        "    \n",
        "    shapes = ['o', '*', 'v', '+']\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#9467bd', '#8c564b',\n",
        "              '#e377c2', '#7f7f7f', '#bcbd22']\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    \n",
        "    for (ax, data, title) in [(ax1, X_train, 'Training Set'), (ax2, X_test, 'Test Set')]:\n",
        "        pred_labels, obj = KM_predictor(data)\n",
        "        print(\"K-Means Objective on \" + title + \": \", obj)\n",
        "        cs = [colors[int(_) % len(colors)] for _ in pred_labels]\n",
        "        ax.scatter(data[:, 0], data[:, 1], alpha=0.5, c=cs)\n",
        "        ax.scatter(KM_centroids[:, 0], KM_centroids[:, 1], marker='o', c='#d62728')\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlim(-12, 12), ax.set_ylim(-12, 12)\n",
        "        ax.set_aspect('equal')    \n",
        "        \n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "    \n",
        "    \n",
        "def show_mog(X_train, X_test, MoG_pi, MoG_centroids, MoG_sigmas, MoG_predictor):\n",
        "    \n",
        "    shapes = ['o', '*', 'v', '+']  \n",
        "    colors = [[31, 119, 180], [255, 127, 14], [44, 160, 44], [148, 103, 189],\n",
        "             [140, 86, 75], [227, 119, 194], [127, 127, 127], [188, 189, 34]]\n",
        "    \n",
        "    max_pi = np.max(MoG_pi)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    \n",
        "    for (ax, data, title) in [(ax1, X_train, 'Training Set'), (ax2, X_test, 'Test Set')]:\n",
        "        pred_labels, norm_llike = MoG_predictor(data)\n",
        "        print(\"MoG Normalized Log-Likelihood \" + title + \": \", norm_llike)\n",
        "        cs = [colors[int(_) % len(colors)] + [0.5*255*MoG_pi[_]/max_pi] for _ in pred_labels]\n",
        "        ax.scatter(data[:, 0], data[:, 1], c=np.array(cs)/255.)\n",
        "        ax.scatter(MoG_centroids[:, 0], MoG_centroids[:, 1], marker='o', c='#d62728')\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlim(-12, 12), ax.set_ylim(-12, 12)\n",
        "        ax.set_aspect('equal')    \n",
        "        \n",
        "        for _ in range(MoG_pi.shape[0]):\n",
        "            plot_ellipse(ax, MoG_centroids[_, :], MoG_sigmas[_, :,:], alpha=MoG_pi[_]/max_pi, color='k')\n",
        "    \n",
        "    plt.show()\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jDGqi5cjNAD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def KMeans(X, K=1):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of a K-means model using training data X\n",
        "        \n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            K: [int] number of clusters to use\n",
        "            \n",
        "        Returns:\n",
        "            centroids: [Kx2] matrix of estimated centroid locations\n",
        "            KMeans_predictor: function taking a matrix inX and returning the\n",
        "                predicted cluster number (starting from 0 to K-1) for each row \n",
        "                of inX; and the normalized KMeans loss (i.e., divided by the\n",
        "                number of rows of inX).\n",
        "                 \n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO  \n",
        "    centroids = np.random.randn(K, X.shape[1])\n",
        "    old_obj = 0\n",
        "    new_obj = 0\n",
        "    while True:\n",
        "        #E-step\n",
        "        dist_matrix = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n",
        "        pred_labels = np.argmin(dist_matrix, axis=1)\n",
        "        #M-step\n",
        "        # use only matrix for get new centroids\n",
        "        centroids = ((X[:, np.newaxis, :] * (pred_labels[:, np.newaxis, np.newaxis] == np.arange(K)[np.newaxis, np.newaxis, :])).sum(axis=0) / (pred_labels[:, np.newaxis, np.newaxis] == np.arange(K)[np.newaxis, np.newaxis, :]).sum(axis=0))\n",
        "\n",
        "        #objective\n",
        "        new_obj = KMeans_objective(X, centroids, pred_labels)\n",
        "        if np.abs(new_obj - old_obj) < 1e-6:\n",
        "            break\n",
        "        old_obj = new_obj\n",
        "     \n",
        "    def KMeans_objective(X, centroids, pred_labels):\n",
        "        dist_matrix = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n",
        "        return np.sum(dist_matrix[np.arange(X.shape[0]), pred_labels]) / X.shape[0]\n",
        "\n",
        "    def KMeans_predictor(inX):\n",
        "        \"\"\"\n",
        "        Use parameters from above to predict cluster for each row of inX\n",
        "        \n",
        "            Inputs:\n",
        "                inX: [mx2] matrix of inputs\n",
        "                \n",
        "            Returns:\n",
        "                pred_labels: [m] array of predicted cluster labels\n",
        "                norm_loss: [float] K-Means loss on inX, i.e. the *squared*\n",
        "                    distance between each point and its associated cluster,\n",
        "                    divided by the number of rows of inX.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO\n",
        "        m = inX.shape[0]\n",
        "        pred_labels = np.random.randint(0, K, m)  # This must be a vector of integers\n",
        "        norm_loss = 0. / m\n",
        "        dist = np.zeros((inX.shape[0], K))\n",
        "        for i in range(K):\n",
        "            dist[:, i] = np.linalg.norm(inX - centroids[i], axis=1)\n",
        "        pred_labels = np.argmin(dist, axis=1)\n",
        "        norm_loss = KMeans_objective(inX, centroids, pred_labels)\n",
        "        return pred_labels, norm_loss\n",
        "        \n",
        "    return centroids, KMeans_predictor\n",
        "\n",
        "def GaussianMixture(X, K=1, use_full_cov=True):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of a Gaussian mixture model using training data X\n",
        "    \n",
        "    **Important:** The locations of the centroids must be initialized using your \n",
        "    K-Means code! With this information, initialize the proportions and variances\n",
        "    accordingly.\n",
        "    \n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            K: [int] number of mixture components to use\n",
        "            use_full_cov: [bool] if True, estimate a full covariance for each \n",
        "                mixture component. Else, we use a scaled identity for each \n",
        "                component (in this case each component might have a \n",
        "                different scaling of the identity: Sigma_i = sigma_i * I).\n",
        "            \n",
        "        Returns:\n",
        "            pi: [K] vector of proportion of each class\n",
        "            centroids: [Kx2] matrix of estimated centroid locations\n",
        "            sigmas: [Kx2x2] tensor of estimated covariance matrices\n",
        "            MoG_predictor: function taking a matrix inX and returning the\n",
        "                predicted cluster number (starting from 0 to K-1) for each row \n",
        "                of inX; and the normalized log-likelihood (i.e., ln(p(inX)) divided by the\n",
        "                number of rows of inX).\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize centroids using KMeans\n",
        "    centroids, _ = KMeans(X, K)\n",
        "    \n",
        "    # TODO\n",
        "\n",
        "    # This is just boiler-plate code. Change this to do the right computations\n",
        "    # but **keep the same names and shape for these variables** \n",
        "    pi = np.ones(K) / K\n",
        "    old_obj = 0\n",
        "    new_obj = 0\n",
        "    \n",
        "     # TODO\n",
        "    if use_full_cov:\n",
        "        # We use a full covariance for each class\n",
        "        sigmas = np.concatenate(K * [np.eye(2)[np.newaxis, ...]], axis=0)\n",
        "    else:\n",
        "        # We use one scaled identity for each class\n",
        "        sigmak2 = np.ones(K)  # These are the sigma^{2}_{k} for k = 1, ..., K\n",
        "        sigmas = np.concatenate([sigmak2[i] * np.eye(2)[np.newaxis, ...] for i in range(K)], axis=0)\n",
        "    \n",
        "    while True:\n",
        "        #E-step\n",
        "        tau = np.zeros((X.shape[0], K))\n",
        "        for i in range(K):\n",
        "            tau[:, i] = pi[i] * np.exp(-0.5 * np.sum(np.matmul((X - centroids[i]), np.linalg.inv(sigmas[i])) * (X - centroids[i]), axis=1)) / np.sqrt(np.linalg.det(sigmas[i]))\n",
        "        tau = tau / np.sum(tau, axis=1, keepdims=True)\n",
        "        \n",
        "        #M-step\n",
        "        for i in range(K):\n",
        "            pi[i] = np.sum(tau[:, i]) / X.shape[0]\n",
        "            centroids[i] = np.sum(tau[:, i][:, np.newaxis] * X, axis=0) / np.sum(tau[:, i])\n",
        "            sigmas[i] = np.sum(tau[:, i][:, np.newaxis, np.newaxis] * np.matmul((X - centroids[i])[:, :, np.newaxis], (X - centroids[i])[:, np.newaxis, :]), axis=0) / np.sum(tau[:, i])\n",
        "        \n",
        "        def GaussianMixture_objective(X, pi, centroids, sigmas):\n",
        "            obj = 0\n",
        "            for i in range(K):\n",
        "                obj += np.sum(pi[i] * np.exp(-0.5 * np.sum(np.matmul((X - centroids[i]), np.linalg.inv(sigmas[i])) * (X - centroids[i]), axis=1)) / np.sqrt(np.linalg.det(sigmas[i])))\n",
        "            return obj / X.shape[0]\n",
        "        \n",
        "        #objective\n",
        "        new_obj = GaussianMixture_objective(X, pi, centroids, sigmas, tau)\n",
        "        if np.abs(new_obj - old_obj) < 1e-6:\n",
        "            break\n",
        "        old_obj = new_obj\n",
        "\n",
        "\n",
        "        \n",
        "    def MoG_predictor(inX):\n",
        "        \"\"\"\n",
        "        # Use parameters from above to predict cluster for each row of inX\n",
        "        \n",
        "            Inputs:\n",
        "                inX: [mx2] matrix of inputs\n",
        "                \n",
        "            Returns:\n",
        "                pred_labels: [m] array of predicted cluster labels\n",
        "                norm_loglike: [float] the log-likelihood of inX, i.e. ln(p(inX)),\n",
        "                    divided by the number of rows of inX.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        m = inX.shape[0]\n",
        "        pred_labels = np.random.randint(0, K, m)  # This must be a vector of integers\n",
        "        norm_loglike = 0. / m\n",
        "        tau = np.zeros((inX.shape[0], K))\n",
        "        for i in range(K):\n",
        "            tau[:, i] = pi[i] * np.exp(-0.5 * np.sum(np.matmul((inX - centroids[i]), np.linalg.inv(sigmas[i])) * (inX - centroids[i]), axis=1)) / np.sqrt(np.linalg.det(sigmas[i]))\n",
        "        tau = tau / np.sum(tau, axis=1, keepdims=True)\n",
        "        pred_labels = np.argmax(tau, axis=1)\n",
        "        # dont use MoG_objective\n",
        "        norm_loglike = np.sum(np.log(np.sum(tau, axis=1))) / inX.shape[0]\n",
        "        return pred_labels, norm_loglike\n",
        "    \n",
        "    return pi, centroids, sigmas, MoG_predictor    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy20jQSD2wH3"
      },
      "source": [
        "## Model Comparison\n",
        "\n",
        "In this section **DO NOT** change the code in any of the cells. Simply answer the questions in the corresponding text cells after having executed your implementation. If you have respected the signature of the functions above in terms of inputs and outputs, your code should run. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvitkVwu3MiW"
      },
      "source": [
        "### K-Means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DujAXlvqsegd"
      },
      "outputs": [],
      "source": [
        "for run in range(3):\n",
        "    KM_centroids, KM_predictor = KMeans(X_train, K=4)\n",
        "    show_kmeans(X_train, X_test, KM_centroids, KM_predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS7z6j9qkHpJ"
      },
      "source": [
        "We have trained your implementation of the K-Means algorithm using `X_train` for 3 different initializations. \n",
        "\n",
        "**Question:** Briefly compare the results above in terms of the location of the centers and the K-means training objective (at convergence) across runs with different initializations. What conclusions can you draw from this? \n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAyEeJ2A4PVG"
      },
      "source": [
        "### EM for Gaussian mixture models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hz51Kf0p2J9"
      },
      "outputs": [],
      "source": [
        "print('Scaled identity covariance matrices')\n",
        "MoG1 = GaussianMixture(X_train, K=4, use_full_cov=False)\n",
        "show_mog(X_train, X_test, MoG1[0], MoG1[1], MoG1[2], MoG1[3])\n",
        "\n",
        "print('\\nFull covariance matrices')\n",
        "MoG2 = GaussianMixture(X_train, K=4, use_full_cov=True)\n",
        "show_mog(X_train, X_test, MoG2[0], MoG2[1], MoG2[2], MoG2[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD_BSuYdtIVM"
      },
      "source": [
        "We have trained your implementation of the EM algorithm for a Gaussian mixture model using `X_train`. The plots show the behavior on the training and testing (left and right) sets when using scaled diagonal or full covariance matrices (top and bottom), respectively. \n",
        "\n",
        "**Question:** Briefly compare the results above in terms of the location of the centers and the normalized log-likelihood between the training and test set. What conclusions can you draw from this? \n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iokK6935t8PE"
      },
      "source": [
        "### Bonus\n",
        "**Question:** From your observations in the K-Means and EM sections, is there any relation one can stablish between these two methods? If so, how do the plots above reflect this?  \n",
        "\n",
        "**Answer:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns6rAGOPwhpI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
